# -*- coding: utf-8 -*-
"""CS772: Review Sentiment_SAKEofDATA_FIlling

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mNpHp7E-WNxCuKN0-5mmVSFA1oxa6aga

Downloading data
"""

import pickle
from collections import defaultdict

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from nltk.tokenize import word_tokenize

print('-'*80)
pickle_in = open("glove_dict_tensor.pkl", "rb")
glove = pickle.load(pickle_in)
print('-'*80)


glove = defaultdict(lambda: torch.tensor(np.zeros(200)), glove)


def pad_sent(sent, n=30):
    return sent[:n] + [-1] * (n - len(sent))


class NET0(nn.Module):
    def __init__(self, input_dim=200*30, numClasses=5):
        super(NET0, self).__init__()
        self.input_dim = input_dim
        self.numClasses = numClasses
        self.output_layer = nn.Linear(self.input_dim, self.numClasses)

    def forward(self, text):
        return self.output_layer(text)


class NET(nn.Module):
    def __init__(self, input_dim=200*30, num_layers=1, activation='sigmoid', numClasses=5):
        super(NET, self).__init__()
        self.input_dim = input_dim
        self.numClasses = numClasses
        if activation == 'sigmoid':
            self.activation = torch.sigmoid
        elif activation == 'relu':
            self.activation = torch.nn.functional.relu
        else:
            self.activation = torch.sigmoid
        self.num_layers = num_layers
        self.neurons = 128
        self.layers = [nn.Linear(self.input_dim, 30*self.neurons)]
        self.neurons = (self.neurons//2)
        self.temp = 0
        for i in range(self.num_layers-1):
            self.layers.append(nn.Linear(30*self.neurons*2, 30*(self.neurons)))
            self.neurons = (self.neurons//2)

        self.layers = torch.nn.ModuleList(self.layers)
        self.output_layer = nn.Linear(30*self.neurons*2, self.numClasses)

    def forward(self, text):
        self.temp = self.activation(self.layers[0](text))
        for i in range(1, self.num_layers):
            self.temp = self.activation(self.layers[i](self.temp))
        return self.output_layer(self.temp)


model = NET(num_layers=1, activation='relu')
device = torch.device("cpu")
model = model.to(device)
fn = nn.CrossEntropyLoss()
fn.to(device)
optimizer = optim.Adam(params=model.parameters(), lr=0.0005)


def demo(sent: str, model):
    with torch.no_grad():
        text = sent
        text = word_tokenize(text)
        text = pad_sent(text)
        text = [glove[i].tolist() for i in text]
        text = torch.tensor(np.array(text).flatten())
        out = model(text.float().to(device))

    preds = nn.Softmax(dim=1)(
        out.detach().cpu().reshape(1, -1)).numpy().tolist()[0]
    return preds


model = NET(num_layers=2, activation='relu')
print('-'*80)


model.load_state_dict(torch.load('models/layer2_relu.pth', map_location='cpu'))
print('-'*80)
print(model)
print(demo("Very good", model))

#torch.save(model.state_dict(), 'layer3_sigmoid.pth')
